{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"./dict/clean_th_lyrics.json\", encoding=\"utf8\") as f:\n",
    "    test_input = json.loads(f.read())\n",
    "\n",
    "print(len(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930\n"
     ]
    }
   ],
   "source": [
    "with open(\"./dict/clean_en_lyrics.json\", encoding=\"utf8\") as f:\n",
    "    test_output = json.loads(f.read())\n",
    "\n",
    "print(len(test_output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data folder is set to `e:\\nlp\\nlp-env\\neuspell\\neuspell\\../data` script\n"
     ]
    }
   ],
   "source": [
    "from model import sefr_cut\n",
    "from model.TH2IPA import TH2IPA\n",
    "from model.IPA2ENG import IPA_matching\n",
    "from neuspell import BertChecker\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model.....\n",
      "Success\n",
      "loading vocab from path:e:\\nlp\\nlp-env\\neuspell\\neuspell\\../data/checkpoints/subwordbert-probwordnoise\\vocab.pkl\n",
      "initializing model\n",
      "Number of parameters in the model: 185211810\n",
      "Loading model params from checkpoint dir: e:\\nlp\\nlp-env\\neuspell\\neuspell\\../data/checkpoints/subwordbert-probwordnoise\n"
     ]
    }
   ],
   "source": [
    "th2ipa = TH2IPA()\n",
    "checker = BertChecker()\n",
    "\n",
    "sefr_cut.SEFR_CUT.load_model(engine='model')\n",
    "checker.from_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_result = []\n",
    "th2ipa_result = []\n",
    "ipa2eng_result = []\n",
    "corrector_result = []\n",
    "\n",
    "all_word = 0\n",
    "correct_word = 0\n",
    "incorrect_word = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integration_pipeline(th_sent, th2ipa, checker):\n",
    "    token_list = sefr_cut.tokenize(th_sent, k=40)[0]\n",
    "    tokenize_result.append(token_list)\n",
    "    ipa_list = th2ipa(token_list)\n",
    "    th2ipa_result.append(ipa_list)\n",
    "    eng_sent = ' '.join([IPA_matching(i)[0] for i in ipa_list]) \n",
    "    ipa2eng_result.append(eng_sent)\n",
    "    correct_sent = checker.correct(eng_sent)\n",
    "    correct_sent = re.sub(' \\' ', '\\'', correct_sent)\n",
    "    corrector_result.append(correct_sent)\n",
    "    return correct_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(all_true, all_pred):\n",
    "    for i in range(len(all_true)):\n",
    "        sent_true = all_true[i]\n",
    "        sent_pred = all_pred[i]\n",
    "\n",
    "        sent_pred = re.sub(' \\' ', '\\'', sent_pred)\n",
    "        sent_pred = re.sub(' \\'', '\\'', sent_pred)\n",
    "        \n",
    "        word_true = sent_true.split()\n",
    "        word_pred = sent_pred.split()\n",
    "\n",
    "        for j in range(word_true):\n",
    "            if word_true[j] == word_pred[j]:\n",
    "                correct_word += 1\n",
    "            else:\n",
    "                incorrect_word += 1\n",
    "            all_word += 1\n",
    "        \n",
    "        accuracy = correct_word / all_word\n",
    "        print('accuracy:', accuracy)\n",
    "        return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 6s 225ms/step\n",
      "1/1 [==============================] - 10s 10s/step\n",
      "2/2 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n"
     ]
    }
   ],
   "source": [
    "for i in test_input[:2]:\n",
    "    integration_pipeline(test_input[0], th2ipa, checker)\n",
    "\n",
    "corrector_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_output, corrector_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
